{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_detector.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN7z52EvdiZmHnE7bHC5+bk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azitt/ML_pytorch_Audioprocessing/blob/main/ML_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For linear layer we need to find the weights (coefficients) of the matrix A and bias b such that the desired output y is approximated, given out \"training set\" x:\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "4k7yTQPljwSJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGMX8tK37um6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "device='cpu'\n",
        "#device='cuda'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinNet, self).__init__()\n",
        "        # Define the model.\n",
        "        self.layer1 = nn.Sequential(nn.Linear(in_features=2, out_features=2, bias=True))\n",
        "        # Generate a fully connected linear neural network model, 1 layer, bias, linear activation function\n",
        "        # returns: Trainable object\n",
        "        #self.act = nn.LeakyReLU() #non-linear activation function\n",
        "        #self.act = nn.ReLU() #non-linear activation function\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        #out = self.act(out) #comment out if not desired\n",
        "        return out"
      ],
      "metadata": {
        "id": "FpG8F4_78DQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data_preparation"
      ],
      "metadata": {
        "id": "tTZc--5m8Qba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input tensor, type torch tensor:\n",
        "#Indices: batch, sample, features or signal dimension. Here: 1 batch, 3 samples, signal dimension 2:\n",
        "\n",
        "#Training set:\n",
        "X=torch.tensor([[1., 2.], [2., 1.],[1., 1.]]).view(1,3,2) #adding the first dimension for the batch\n",
        "print(\"X.shape\", X.shape)\n",
        "\n",
        "#Target:\n",
        "Y=torch.tensor([[1., 0.], [0., 1.],[0., 0.]]).view(1,3,2)\n",
        "print(\"Y.shape\", Y.shape)\n",
        "\n",
        "#Validation set, to test generalization:\n",
        "Xval=torch.tensor([[0.5, 1.0], [1., 0.5],[0.5, 0.5]]).view(1,3,2)\n",
        "#Validation Target:\n",
        "Yval=torch.tensor([[1., 0.], [0., 1.],[0., 0.]]).view(1,3,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNG7TZr98Tip",
        "outputId": "2ef9e5e5-99b0-4de8-ca29-8b9bf846be1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape torch.Size([1, 3, 2])\n",
            "Y.shape torch.Size([1, 3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create network object:\n",
        "model = LinNet().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "print(\"Define loss function:\", loss_fn)\n",
        "#learning_rate = 1e-4\n",
        "#optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
        "print(\"Define optimizer:\", optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3KJ3fPW8s87",
        "outputId": "33776a24-8f6b-482e-c7ea-6eaa6fb483ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define loss function: MSELoss()\n",
            "Define optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10000):\n",
        "    Ypred=model(X) #the model produces prediction output\n",
        "    loss=loss_fn(Ypred, Y) #prediction and target compared by loss\n",
        "    if epoch%1000==0:\n",
        "        print(epoch, loss.item()) #print current loss value\n",
        "    optimizer.zero_grad() #optimizer sets previous gradients to zero\n",
        "    loss.backward() #optimizer computes new gradients\n",
        "    optimizer.step() #optimizer updates weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W5YkwgZ80jJ",
        "outputId": "198e5e14-90d8-4a43-8916-b4872d16d73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.9826375246047974\n",
            "1000 0.0002927322348114103\n",
            "2000 2.4277860575239174e-06\n",
            "3000 2.016355260536784e-08\n",
            "4000 1.6898364652018216e-10\n",
            "5000 4.4426684553400264e-12\n",
            "6000 4.4426684553400264e-12\n",
            "7000 4.4426684553400264e-12\n",
            "8000 4.4426684553400264e-12\n",
            "9000 4.4426684553400264e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred=model(X) # Make Predictions based on the obtained weights\n",
        "loss=loss_fn(Ypred, Y)\n",
        "print(\"Loss on trainig set:\", loss)\n",
        "Yvalpred=model(Xval) # Make Predictions based on the obtained weights\n",
        "loss=loss_fn(Yvalpred, Yval)\n",
        "print(\"Loss on validation set:\", loss)\n",
        "weights = model.state_dict() #read obtained weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaaQiyMN9KqZ",
        "outputId": "0f64afe9-9d55-41f9-99ed-50e338ff4050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on trainig set: tensor(4.4427e-12, grad_fn=<MseLossBackward0>)\n",
            "Loss on validation set: tensor(0.5000, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss values: Only **4.6493e-12** on the **training se**t, but **0.5000** on the **validation** set Hence we have a **bad generalization**."
      ],
      "metadata": {
        "id": "FBBt6Vwb9_qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*We add activation function and train the model and check the output*****"
      ],
      "metadata": {
        "id": "Job9bPZ1Ay2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinNet2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinNet2, self).__init__()\n",
        "        # Define the model.\n",
        "        self.layer1 = nn.Sequential(nn.Linear(in_features=2, out_features=2, bias=True))\n",
        "        # Generate a fully connected linear neural network model, 1 layer, bias, linear activation function\n",
        "        # returns: Trainable object\n",
        "        self.act = nn.LeakyReLU() #non-linear activation function\n",
        "        #self.act = nn.ReLU() #non-linear activation function\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.act(out) #comment out if not desired\n",
        "        return out"
      ],
      "metadata": {
        "id": "a0crL1KL-QDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create network object:\n",
        "model = LinNet2().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "print(\"Define loss function:\", loss_fn)\n",
        "#learning_rate = 1e-4\n",
        "#optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
        "print(\"Define optimizer:\", optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HniLGB9ZAtbB",
        "outputId": "11f01b64-875c-4c82-a7ca-a310bb7f007e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define loss function: MSELoss()\n",
            "Define optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10000):\n",
        "    Ypred=model(X) #the model produces prediction output\n",
        "    loss=loss_fn(Ypred, Y) #prediction and target compared by loss\n",
        "    if epoch%1000==0:\n",
        "        print(epoch, loss.item()) #print current loss value\n",
        "    optimizer.zero_grad() #optimizer sets previous gradients to zero\n",
        "    loss.backward() #optimizer computes new gradients\n",
        "    optimizer.step() #optimizer updates weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlh2T_aZCBeY",
        "outputId": "5cc1e571-d648-46ec-ea73-37491ac33184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.3932311236858368\n",
            "1000 1.4584293239749968e-05\n",
            "2000 1.4528071005770471e-05\n",
            "3000 1.4480231584457215e-05\n",
            "4000 1.4432474017667118e-05\n",
            "5000 1.438479739590548e-05\n",
            "6000 1.4337198081193492e-05\n",
            "7000 1.4289677892520558e-05\n",
            "8000 1.4242238648876082e-05\n",
            "9000 1.4194876712281257e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred=model(X) # Make Predictions based on the obtained weights\n",
        "loss=loss_fn(Ypred, Y)\n",
        "print(\"Loss on trainig set:\", loss)\n",
        "Yvalpred=model(Xval) # Make Predictions based on the obtained weights\n",
        "loss=loss_fn(Yvalpred, Yval)\n",
        "print(\"Loss on validation set:\", loss)\n",
        "weights = model.state_dict() #read obtained weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKK7cfl9AwFX",
        "outputId": "e82fbad6-05f8-46bd-b579-08970b806419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on trainig set: tensor(1.4148e-05, grad_fn=<MseLossBackward0>)\n",
            "Loss on validation set: tensor(0.1697, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss values: Only **1.4148e-05** on the **training set**, but **0.1697** on the **validation set** Hence we have a better generalization than first model."
      ],
      "metadata": {
        "id": "iZZi7IYrCV52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the effect of vanishing gradients on the optimization, we uncomment the line with \"self.act=nn.ReLU()\".\n",
        "**This activation function has a constant \"0\" for negative values and hence a vanishing gradient for negative values.** "
      ],
      "metadata": {
        "id": "VDzpCqlkCv04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinNet3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinNet3, self).__init__()\n",
        "        # Define the model.\n",
        "        self.layer1 = nn.Sequential(nn.Linear(in_features=2, out_features=2, bias=True))\n",
        "        # Generate a fully connected linear neural network model, 1 layer, bias, linear activation function\n",
        "        # returns: Trainable object\n",
        "        #self.act = nn.LeakyReLU() #non-linear activation function\n",
        "        self.act = nn.ReLU() #non-linear activation function\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.act(out) #comment out if not desired\n",
        "        return out"
      ],
      "metadata": {
        "id": "JXkvaGdWClFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create network object:\n",
        "model = LinNet3().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "print(\"Define loss function:\", loss_fn)\n",
        "#learning_rate = 1e-4\n",
        "#optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.1)\n",
        "print(\"Define optimizer:\", optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33gXZF5YDroX",
        "outputId": "fc458262-01c6-4c75-ea29-62cd2d7a1c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define loss function: MSELoss()\n",
            "Define optimizer: SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    lr: 0.1\n",
            "    momentum: 0\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10000):\n",
        "    Ypred=model(X) #the model produces prediction output\n",
        "    loss=loss_fn(Ypred, Y) #prediction and target compared by loss\n",
        "    if epoch%1000==0:\n",
        "        print(epoch, loss.item()) #print current loss value\n",
        "    optimizer.zero_grad() #optimizer sets previous gradients to zero\n",
        "    loss.backward() #optimizer computes new gradients\n",
        "    optimizer.step() #optimizer updates weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcl-1YaqDvs2",
        "outputId": "396fbaa7-f949-4bb1-ba99-5e83959a271a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.4259568750858307\n",
            "1000 0.3333333432674408\n",
            "2000 0.3333333432674408\n",
            "3000 0.3333333432674408\n",
            "4000 0.3333333432674408\n",
            "5000 0.3333333432674408\n",
            "6000 0.3333333432674408\n",
            "7000 0.3333333432674408\n",
            "8000 0.3333333432674408\n",
            "9000 0.3333333432674408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ypred=model(X) # Make Predictions based on the obtained weights\n",
        "loss=loss_fn(Ypred, Y)\n",
        "print(\"Loss on trainig set:\", loss)\n",
        "Yvalpred=model(Xval) # Make Predictions based on the obtained weights\n",
        "loss=loss_fn(Yvalpred, Yval)\n",
        "print(\"Loss on validation set:\", loss)\n",
        "weights = model.state_dict() #read obtained weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD_N2m0vD1kn",
        "outputId": "575df637-f3fb-4f4b-a299-d455de54501c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on trainig set: tensor(0.3333, grad_fn=<MseLossBackward0>)\n",
            "Loss on validation set: tensor(0.3600, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **optimizer gets stuck at a loss value of 0.33** during training. This is cause by the vanishing gradient of the ReLU function in the negative input range."
      ],
      "metadata": {
        "id": "In5fXYQDEa5W"
      }
    }
  ]
}